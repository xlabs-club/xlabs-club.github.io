<!doctype html><html lang=zh data-bs-theme=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=preload href=https://www.xlabs.club/fonts/vendor/jost/jost-v4-latin-regular.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=https://www.xlabs.club/fonts/vendor/jost/jost-v4-latin-500.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=https://www.xlabs.club/fonts/vendor/jost/jost-v4-latin-700.woff2 as=font type=font/woff2 crossorigin><link rel=preconnect href=https://KMWY81ZWS3-dsn.algolia.net crossorigin><script src=/js/color-mode.86a91f050a481d0a3f0c72ac26543cb6228c770875981c58dcbc008fd3f875c8.js integrity="sha256-hqkfBQpIHQo/DHKsJlQ8tiKMdwh1mBxY3LwAj9P4dcg="></script><link rel=stylesheet href="/main.c4fdc4149e5e1c7b1e9f189cefc6eb2be050c2cfb72145725c54e0862ca767949ef61742676a78018768d0816d71f9004da915f293da8a8ba2666dd8b1062094.css" integrity="sha512-xP3EFJ5eHHsenxic78brK+BQws+3IUVyXFTghiynZ5Se9hdCZ2p4AYdo0IFtcfkATakV8pPaiouiZm3YsQYglA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><base href=https://www.xlabs.club/blog/java-memory/><link rel=canonical href=https://www.xlabs.club/blog/java-memory/><title>K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位</title>
<meta name=description content="K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位，不同 GC 效果对比"><link rel=icon href=/favicon.ico sizes=32x32><link rel=icon href=/favicon.svg type=image/svg+xml><link rel=apple-touch-icon href=/apple-touch-icon.png sizes=180x180 type=image/png><link rel=icon href=/favicon-192x192.png sizes=192x192 type=image/png><link rel=icon href=/favicon-512x512.png sizes=512x512 type=image/png><link rel=manifest href=/manifest.webmanifest><meta property="og:url" content="https://www.xlabs.club/blog/java-memory/"><meta property="og:site_name" content="XLabs"><meta property="og:title" content="K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位"><meta property="og:description" content="K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位，不同 GC 效果对比"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-01-07T10:54:37+08:00"><meta property="article:modified_time" content="2025-06-05T20:15:24+08:00"><meta property="article:tag" content="K8s"><meta property="article:tag" content="Java"><meta property="og:image" content="https://www.xlabs.club/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.xlabs.club/cover.png"><meta name=twitter:title content="K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位"><meta name=twitter:description content="K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位，不同 GC 效果对比"><meta name=twitter:site content="@xlabs-club"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://www.xlabs.club/","name":"卫星实验室","position":1},{"@type":"ListItem","item":"https://www.xlabs.club/blog/","name":"Blog","position":2},{"@type":"ListItem","name":"K8 S Pod 容器内 Java 进程内存分析，内存虚高以及容器 Oom 或 Jave Oom 问题定位","position":3}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"BlogPosting","headline":"K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位","description":"K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位，不同 GC 效果对比","isPartOf":{"@id":"https://www.xlabs.club/blog/java-memory/"},"mainEntityOfPage":{"@id":"https://www.xlabs.club/blog/java-memory/"},"datePublished":"2023-01-07T10:54:37+08:00","dateModified":"2025-06-05T20:15:24+08:00","image":[{"@id":"https://www.xlabs.club/blog/java-memory/after-jemalloc.png"},{"@id":"https://www.xlabs.club/blog/java-memory/before-jemalloc.png"},{"@id":"https://www.xlabs.club/blog/java-memory/grafana-pod-jvm.png"},{"@id":"https://www.xlabs.club/blog/java-memory/java-heap-use.png"},{"@id":"https://www.xlabs.club/blog/java-memory/jemalloc-jvm.png"},{"@id":"https://www.xlabs.club/blog/java-memory/jvm-jemalloc.svg"},{"@id":"https://www.xlabs.club/blog/java-memory/jvm-memory-structure.png"},{"@id":"https://www.xlabs.club/blog/java-memory/memory-dance.png"}],"author":{"@type":"Organization","name":"XLabs Club 卫星实验室","url":"https://www.xlabs.club/"},"publisher":{"@type":"Organization","name":"XLabs Club 卫星实验室"}}]}</script><meta http-equiv=content-language content='zh-cn'></head><body class="single section blog" data-bs-spy=scroll data-bs-target=#toc data-bs-root-margin="0px 0px -60%" data-bs-smooth-scroll=true tabindex=0><div class=sticky-top><header class="navbar navbar-expand-lg"><div class=container-fluid><a class="navbar-brand me-auto me-lg-3" href=/>XLabs</a><div id=docsearch class=d-none tabindex=-1 aria-disabled=true></div><button type=button id=searchToggleMobile class="btn btn-link nav-link mx-2 d-lg-none" aria-label="Search website">
<svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
</button>
<button class="btn btn-link nav-link mx-2 order-3 d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasNavMain aria-controls=offcanvasNavMain aria-label="Open main navigation menu"><svg class="icon icon-tabler icon-tabler-menu" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="4" y1="8" x2="20" y2="8"/><line x1="4" y1="16" x2="20" y2="16"/></svg></button><div class="offcanvas offcanvas-end h-auto" tabindex=-1 id=offcanvasNavMain aria-labelledby=offcanvasNavMainLabel><div class=offcanvas-header><h5 class=offcanvas-title id=offcanvasNavMainLabel>XLabs</h5><button type=button class="btn btn-link nav-link p-0 ms-auto" data-bs-dismiss=offcanvas aria-label=Close><svg class="icon icon-tabler icon-tabler-x" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 6 6 18"/><path d="M6 6l12 12"/></svg></button></div><div class="offcanvas-body d-flex flex-column flex-lg-row justify-content-between"><ul class="navbar-nav flex-grow-1"><li class=nav-item><a class=nav-link href=https://www.xlabs.club/docs/guides/introduction/>Docs</a></li><li class=nav-item><a class="nav-link active" href=https://www.xlabs.club/blog/ aria-current=true>Blog</a></li><li class=nav-item><a class=nav-link href=https://www.xlabs.club/blog/xlabs-link-exchange>Links</a></li><li class=nav-item><a class=nav-link href=https://www.xlabs.club/about/>About</a></li></ul><button type=button id=searchToggleDesktop class="btn btn-link nav-link mx-2 d-none d-lg-block" aria-label="Search website">
<svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
</button>
<button id=buttonColorMode class="btn btn-link mx-auto nav-link p-0 ms-lg-2 me-lg-1" type=button aria-label="Toggle theme"><svg data-bs-theme-value="dark" class="icon icon-tabler icon-tabler-moon" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132.0.263.0.393.0a7.5 7.5.0 007.92 12.446A9 9 0 1112 2.992z"/></svg><svg data-bs-theme-value="light" class="icon icon-tabler icon-tabler-sun" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 12m-4 0a4 4 0 108 0 4 4 0 10-8 0m-5 0h1m8-9v1m8 8h1m-9 8v1M5.6 5.6l.7.7m12.1-.7-.7.7m0 11.4.7.7m-12.1-.7-.7.7"/></svg></button><ul id=socialMenu class="nav mx-auto flex-row order-lg-4"><li class=nav-item><a class="nav-link social-link" href=https://github.com/xlabs-club/xlabs-club.github.io><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg><small class="ms-2 visually-hidden">GitHub</small></a></li></ul></div></div></div></header></div><div class="wrap container-fluid" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col container-fw d-lg-flex flex-lg-row justify-content-center mx-auto"><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>目录</h3><nav id=toc><ul><li><a href=#统计指标>统计指标</a></li><li><a href=#java-进程内存分析>Java 进程内存分析</a></li><li><a href=#jemalloc>jemalloc</a></li><li><a href=#内存交还给操作系统>内存交还给操作系统</a></li><li><a href=#内存分析工具速览>内存分析工具速览</a><ul><li><a href=#pmap-查看内存内容>pmap 查看内存内容</a></li><li><a href=#识别-linux-节点上的-cgroup-版本>识别 Linux 节点上的 cgroup 版本</a></li></ul></li><li><a href=#问题原因分析和调整>问题原因分析和调整</a></li><li><a href=#不同-jvm-参数内存占用对比>不同 JVM 参数内存占用对比</a></li><li><a href=#java-分析工具>Java 分析工具</a></li><li><a href=#参考资料>参考资料</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9 mx-xl-auto"><h1>K8S Pod 容器内 Java 进程内存分析，内存虚高以及容器 OOM 或 Jave OOM 问题定位</h1><nav class="toc-mobile d-xl-none" aria-label="Quaternary navigation"><details><summary>目录</summary><div class=page-links><nav id=TableOfContents><ul><li><a href=#统计指标>统计指标</a></li><li><a href=#java-进程内存分析>Java 进程内存分析</a></li><li><a href=#jemalloc>jemalloc</a></li><li><a href=#内存交还给操作系统>内存交还给操作系统</a></li><li><a href=#内存分析工具速览>内存分析工具速览</a><ul><li><a href=#pmap-查看内存内容>pmap 查看内存内容</a></li><li><a href=#识别-linux-节点上的-cgroup-版本>识别 Linux 节点上的 cgroup 版本</a></li></ul></li><li><a href=#问题原因分析和调整>问题原因分析和调整</a></li><li><a href=#不同-jvm-参数内存占用对比>不同 JVM 参数内存占用对比</a></li><li><a href=#java-分析工具>Java 分析工具</a></li><li><a href=#参考资料>参考资料</a></li></ul></nav></div></details></nav><p>故事背景：</p><p>一个 K8S Pod，里面只有一个 Java 进程，K8S request 和 limit memory 都是 2G，Java 进程核心参数包括：<code>-XX:+UseZGC -Xmx1024m -Xms768m</code>。</p><p>服务启动一段时间后，查看 Grafana 监控数据，Pod 内存使用量约 1.5G，JVM 内存使用量约 500M，通过 jvm dump 分析没有任何大对象，运行三五天后出现 K8S Container OOM。</p><p>首先区分下 Container OOM 和 Jvm OOM，Container OOM 是 Pod 内进程申请内存大约 K8S Limit 所致。</p><p>问题来了：</p><ol><li>Pod 2G 内存，JVM 设置了 <code>Xmx 1G</code>，已经预留了 1G 内存，为什么还会 Container OOM，这预留的 1G 内存被谁吃了。</li><li>正常情况下（无 Container OOM），Grafana 看到的监控数据，Pod 内存使用量 1.5G， JVM 内存使用量 500M，差别为什么这么大。</li><li>Pod 内存使用量为什么超过 Xmx 限制。</li></ol><p>Grafana 监控图。</p><p><a href=./grafana-pod-jvm.png><img src=/blog/java-memory/grafana-pod-jvm_hu_f3e003bf88afca3a.webp width=1920 height=3600 decoding=async fetchpriority=auto loading=lazy alt="Grafana 监控图" id=h-rh-i-0></a></p><h2 id=统计指标>统计指标<a href=#统计指标 class=anchor aria-hidden=true>#</a></h2><p><code>Pod 内存使用量</code>统计的指标是 <code>container_memory_working_set_bytes</code>：</p><ul><li>container_memory_usage_bytes = container_memory_rss + container_memory_cache + kernel memory</li><li>container_memory_working_set_bytes = container_memory_usage_bytes - total_inactive_file（未激活的匿名缓存页）</li></ul><p>container_memory_working_set_bytes 是容器真实使用的内存量，也是资源限制 limit 时的 OOM 判断依据。</p><p>另外注意 cgroup 版本差异： <code>container_memory_cache</code> reflects <code>cache (cgroup v1)</code> or <code>file (cgroup v2)</code> entry in memory.stat.</p><p><code>JVM 内存使用量</code>统计的指标是 <code>jvm_memory_bytes_used</code>： heap、non-heap 以及<code>其他</code> 真实用量总和。下面解释其他。</p><p>首先说结论：在 POD 内，通过 top、free 看到的指标都是不准确的，不用看了，如果要看真实的数据以 cgroup 为准。</p><p>container_memory_working_set_bytes 指标来自 cadvisor，cadvisor 数据来源 cgroup，可以查看以下文件获取真实的内存情况。</p><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> cgroup v2 文件地址
</span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>ll /sys/fs/cgroup/memory.*
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.current
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.events
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.events.local
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.high
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.low
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.max
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.min
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.numa_stat
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.oom.group
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.pressure
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.stat
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.current
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.events
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.high
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.max
</span></span></span></code></pre></div></figure></div><p>JVM 关于使用量和提交量的解释。</p><p><code>Used Size</code>：The used space is the amount of memory that is currently occupied by Java objects.
当前实际真的用着的内存，每个 bit 都对应了有值的。</p><p><code>Committed Size</code>：The committed size is the amount of memory guaranteed to be available for use by the Java virtual machine.
操作系统向 JVM 保证可用的内存大小，或者说 JVM 向操作系统已经要的内存。站在操作系统的角度，就是已经分出去（占用）的内存，保证给 JVM 用了，其他进程不能用了。 由于操作系统的内存管理是惰性的，对于已申请的内存虽然会分配地址空间，但并不会直接占用物理内存，真正使用的时候才会映射到实际的物理内存，所以 committed > res 也是很可能的。</p><h2 id=java-进程内存分析>Java 进程内存分析<a href=#java-进程内存分析 class=anchor aria-hidden=true>#</a></h2><p>Pod 的内存使用量 1.5G，都包含哪些。</p><p>kernel memory 为 0，Cache 约 1100M，rss 约 650M，inactive_file 约 200M。可以看到 Cache 比较大，因为这个服务比较特殊有很多文件操作。</p><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> cgroup v2 变量变了
</span></span><span class=line><span class=cl><span class=go>cat /sys/fs/cgroup/memory.stat
</span></span></span><span class=line><span class=cl><span class=go>anon 846118912
</span></span></span><span class=line><span class=cl><span class=go>file 2321530880
</span></span></span><span class=line><span class=cl><span class=go>kernel_stack 10895360
</span></span></span><span class=line><span class=cl><span class=go>pagetables 15523840
</span></span></span><span class=line><span class=cl><span class=go>percpu 0
</span></span></span><span class=line><span class=cl><span class=go>sock 1212416
</span></span></span><span class=line><span class=cl><span class=go>shmem 1933574144
</span></span></span><span class=line><span class=cl><span class=go>file_mapped 1870290944
</span></span></span><span class=line><span class=cl><span class=go>file_dirty 12288
</span></span></span><span class=line><span class=cl><span class=go>file_writeback 0
</span></span></span><span class=line><span class=cl><span class=go>swapcached 0
</span></span></span><span class=line><span class=cl><span class=go>anon_thp 0
</span></span></span><span class=line><span class=cl><span class=go>file_thp 0
</span></span></span><span class=line><span class=cl><span class=go>shmem_thp 0
</span></span></span><span class=line><span class=cl><span class=go>inactive_anon 2602876928
</span></span></span><span class=line><span class=cl><span class=go>active_anon 176771072
</span></span></span><span class=line><span class=cl><span class=go>inactive_file 188608512
</span></span></span><span class=line><span class=cl><span class=go>active_file 199348224
</span></span></span><span class=line><span class=cl><span class=go>unevictable 0
</span></span></span><span class=line><span class=cl><span class=go>slab_reclaimable 11839688
</span></span></span><span class=line><span class=cl><span class=go>slab_unreclaimable 7409400
</span></span></span><span class=line><span class=cl><span class=go>slab 19249088
</span></span></span><span class=line><span class=cl><span class=go>workingset_refault_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_refault_file 318
</span></span></span><span class=line><span class=cl><span class=go>workingset_activate_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_activate_file 95
</span></span></span><span class=line><span class=cl><span class=go>workingset_restore_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_restore_file 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_nodereclaim 0
</span></span></span><span class=line><span class=cl><span class=go>pgfault 2563565
</span></span></span><span class=line><span class=cl><span class=go>pgmajfault 15
</span></span></span><span class=line><span class=cl><span class=go>pgrefill 14672
</span></span></span><span class=line><span class=cl><span class=go>pgscan 25468
</span></span></span><span class=line><span class=cl><span class=go>pgsteal 25468
</span></span></span><span class=line><span class=cl><span class=go>pgactivate 106436
</span></span></span><span class=line><span class=cl><span class=go>pgdeactivate 14672
</span></span></span><span class=line><span class=cl><span class=go>pglazyfree 0
</span></span></span><span class=line><span class=cl><span class=go>pglazyfreed 0
</span></span></span><span class=line><span class=cl><span class=go>thp_fault_alloc 0
</span></span></span><span class=line><span class=cl><span class=go>thp_collapse_alloc 0
</span></span></span></code></pre></div></figure></div><p>通过 Java 自带的 Native Memory Tracking 看下内存提交量。</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Java 启动时先打开 NativeMemoryTracking，默认是关闭的。注意不要在生产环境长期开启，有性能损失</span>
</span></span><span class=line><span class=cl>java -XX:NativeMemoryTracking<span class=o>=</span>detail -jar
</span></span><span class=line><span class=cl><span class=c1># 查看详情</span>
</span></span><span class=line><span class=cl>jcmd <span class=k>$(</span>pgrep java<span class=k>)</span> VM.native_memory detail <span class=nv>scale</span><span class=o>=</span>MB
</span></span><span class=line><span class=cl><span class=c1># 查看 summary</span>
</span></span><span class=line><span class=cl>jcmd <span class=k>$(</span>pgrep java<span class=k>)</span> VM.native_memory summary <span class=nv>scale</span><span class=o>=</span>MB
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建基线，然后分不同时间进行 diff 查看变化</span>
</span></span><span class=line><span class=cl>jcmd <span class=k>$(</span>pgrep java<span class=k>)</span> VM.native_memory baseline
</span></span><span class=line><span class=cl>jcmd <span class=k>$(</span>pgrep java<span class=k>)</span> VM.native_memory detail.diff <span class=nv>scale</span><span class=o>=</span>MB
</span></span><span class=line><span class=cl>jcmd <span class=k>$(</span>pgrep java<span class=k>)</span> VM.native_memory summary.diff <span class=nv>scale</span><span class=o>=</span>MB</span></span></code></pre></div></figure></div><p>通过 Native Memory Tracking 追踪到的详情大致如下，关注其中每一项 <code>committed</code> 值。</p><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Native Memory Tracking:
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>(Omitting categories weighting less than 1MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Total: reserved=68975MB, committed=1040MB
</span></span></span><span class=line><span class=cl><span class=go>-                 Java Heap (reserved=58944MB, committed=646MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=58944MB, committed=646MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                     Class (reserved=1027MB, committed=15MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (classes #19551)  #加载类的个数
</span></span></span><span class=line><span class=cl><span class=go>                            (  instance classes #18354, array classes #1197)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=3MB #63653)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=1024MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (  Metadata:   )
</span></span></span><span class=line><span class=cl><span class=go>                            (    reserved=96MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    used=93MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    waste=0MB =0.40%)
</span></span></span><span class=line><span class=cl><span class=go>                            (  Class space:)
</span></span></span><span class=line><span class=cl><span class=go>                            (    reserved=1024MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    used=11MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    waste=1MB =4.63%)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Thread (reserved=337MB, committed=37MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (thread #335) #线程的个数
</span></span></span><span class=line><span class=cl><span class=go>                            (stack: reserved=336MB, committed=36MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #2018)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                      Code (reserved=248MB, committed=86MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=6MB #24750)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=242MB, committed=80MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                        GC (reserved=8243MB, committed=83MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=19MB #45814)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=8224MB, committed=64MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                  Compiler (reserved=3MB, committed=3MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=3MB #2212)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                  Internal (reserved=7MB, committed=7MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=7MB #31683)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                     Other (reserved=18MB, committed=18MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=18MB #663)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Symbol (reserved=19MB, committed=19MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=17MB #502325)
</span></span></span><span class=line><span class=cl><span class=go>                            (arena=2MB #1)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-    Native Memory Tracking (reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #8073)
</span></span></span><span class=line><span class=cl><span class=go>                            (tracking overhead=11MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-        Shared class space (reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Module (reserved=1MB, committed=1MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #4996)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-           Synchronization (reserved=1MB, committed=1MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #2482)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                 Metaspace (reserved=97MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #662)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=96MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-           Object Monitors (reserved=8MB, committed=8MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=8MB #39137)
</span></span></span></code></pre></div></figure></div><p>先解释下内存参数意义：</p><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>reserved：JVM 向操作系统预约的虚拟内存地址空间，仅是地址空间预留，不消耗物理资源，防止其他进程使用这段地址范围，类似&#34;土地规划许可&#34;，但尚未建房。
</span></span></span><span class=line><span class=cl><span class=go>committed：JVM 实际分配的物理内存（RAM + Swap），真实消耗系统内存资源。
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>虚拟地址空间
</span></span></span><span class=line><span class=cl><span class=go>┌──────────────────────────────┐
</span></span></span><span class=line><span class=cl><span class=go>│         reserved=16MB        │  ← 整个预约区域
</span></span></span><span class=line><span class=cl><span class=go>├───────────────┬──────────────┤
</span></span></span><span class=line><span class=cl><span class=go>│ committed=13MB│ 未使用 3MB   │  ← 实际使用的物理内存
</span></span></span><span class=line><span class=cl><span class=go>└───────────────┴──────────────┘
</span></span></span></code></pre></div></figure></div><p><a href=./jvm-memory-structure.png><img src=/blog/java-memory/jvm-memory-structure_hu_4f786b9faff680bc.webp width=852 height=826 decoding=async fetchpriority=auto loading=lazy alt="JVM 内存结构" id=h-rh-i-1></a></p><ul><li><p>Heap
Heap 是 Java 进程中使用量最大的一部分内存，是最常遇到内存问题的部分，Java 也提供了很多相关工具来排查堆内存泄露问题，这里不详细展开。Heap 与 RSS 相关的几个重要 JVM 参数如下：
Xms：Java Heap 初始内存大小。（目前我们用的百分比控制，MaxRAMPercentage)
Xmx：Java Heap 的最大大小。(InitialRAMPercentage)
XX:+UseAdaptiveSizePolicy：是否开启自适应大小策略。开启后，JVM 将动态判断是否调整 Heap size，来降低系统负载。</p></li><li><p>Metaspace
Metaspace 主要包含方法的字节码，Class 对象，常量池。一般来说，记载的类越多，Metaspace 使用的内存越多。与 Metaspace 相关的 JVM 参数有：
XX:MaxMetaspaceSize: 最大的 Metaspace 大小限制【默认无限制】
XX:MetaspaceSize=64M: 初始的 Metaspace 大小。如果 Metaspace 空间不足，将会触发 Full GC。
类空间占用评估，给两个数字可供参考：10K 个类约 90M，15K 个类约 100M。
什么时候回收：分配给一个类的空间，是归属于这个类的类加载器的，只有当这个类加载器卸载的时候，这个空间才会被释放。释放 Metaspace 的空间，并不意味着将这部分空间还给系统内存，这部分空间通常会被 JVM 保留下来。
扩展：参考资料中的<code>Java Metaspace 详解</code>，这里完美解释 Metaspace、Compressed Class Space 等。</p></li><li><p>Thread
NMT 中显示的 Thread 部分内存与线程数与 -Xss 参数成正比，一般来说 committed 内存等于 <code>Xss *线程数</code> 。</p></li><li><p>Code
JIT 动态编译产生的 Code 占用的内存。这部分内存主要由-XX:ReservedCodeCacheSize 参数进行控制。</p></li><li><p>Internal
Internal 包含命令行解析器使用的内存、JVMTI、PerfData 以及 Unsafe 分配的内存等等。
需要注意的是，Unsafe_AllocateMemory 分配的内存在 JDK11 之前，在 NMT 中都属于 Internal，但是在 JDK11 之后被 NMT 归属到 Other 中。</p></li><li><p>Symbol
Symbol 为 JVM 中的符号表所使用的内存，HotSpot 中符号表主要有两种：SymbolTable 与 StringTable。
大家都知道 Java 的类在编译之后会生成 Constant pool 常量池，常量池中会有很多的字符串常量，HotSpot 出于节省内存的考虑，往往会将这些字符串常量作为一个 Symbol 对象存入一个 HashTable 的表结构中即 SymbolTable，如果该字符串可以在 SymbolTable 中 lookup（SymbolTable::lookup）到，那么就会重用该字符串，如果找不到才会创建新的 Symbol（SymbolTable::new_symbol）。
当然除了 SymbolTable，还有它的双胞胎兄弟 StringTable（StringTable 结构与 SymbolTable 基本是一致的，都是 HashTable 的结构），即我们常说的字符串常量池。平时做业务开发和 StringTable 打交道会更多一些，HotSpot 也是基于节省内存的考虑为我们提供了 StringTable，我们可以通过 String.intern 的方式将字符串放入 StringTable 中来重用字符串。</p></li><li><p>Native Memory Tracking
Native Memory Tracking 使用的内存就是 JVM 进程开启 NMT 功能后，NMT 功能自身所申请的内存。</p></li></ul><p>观察上面几个区域的分配，没有明显的异常。</p><p>NMT 追踪到的 是 Committed，不一定是 Used，NMT 和 cadvisor 没有找到必然的对应的关系。可以参考 RSS，cadvisor 追踪到 RSS 是 650M，JVM Used 是 500M，还有大约 150M 浮动到哪里去了。</p><p>因为 NMT 只能 Track JVM 自身的内存分配情况，比如：Heap 内存分配，direct byte buffer 等。无法追踪的情况主要包括：</p><ul><li>使用 JNI 调用的一些第三方 native code 申请的内存，比如使用 System.Loadlibrary 加载的一些库。</li><li>标准的 Java Class Library，典型的，如文件流等相关操作（如：Files.list、ZipInputStream 和 DirectoryStream 等）。主要涉及到的调用是 Unsafe.allocateMemory 和 java.util.zip.Inflater.init(Native Method)。</li></ul><p>怎么追踪 NMT 追踪不到的<code>其他内存</code>，目前是安装了 jemalloc 内存分析工具，他能追踪底层内存的分配情况输出报告。</p><p>通过 jemalloc 内存分析工具佐证了上面的结论，Unsafe.allocateMemory 和 java.util.zip.Inflater.init 占了 30%，基本吻合。</p><p><a href=./jemalloc-jvm.png><img src=/blog/java-memory/jemalloc-jvm_hu_4355d9170521b05.webp width=1076 height=866 decoding=async fetchpriority=auto loading=lazy alt="Jemalloc 内存结果" id=h-rh-i-2></a></p><p>启动 arthas 查看下类调用栈，在 arthas 里执行以下命令：</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 先设置 unsafe true</span>
</span></span><span class=line><span class=cl>options unsafe <span class=nb>true</span>
</span></span><span class=line><span class=cl><span class=c1># 这个没有</span>
</span></span><span class=line><span class=cl>stack sun.misc.Unsafe allocateMemory
</span></span><span class=line><span class=cl><span class=c1># 这个有</span>
</span></span><span class=line><span class=cl>stack jdk.internal.misc.Unsafe allocateMemory
</span></span><span class=line><span class=cl>stack java.util.zip.Inflater inflate
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># stack 经常追踪不到，改用 profiler 输出内存分配火焰图</span>
</span></span><span class=line><span class=cl>profiler start --event alloc --duration <span class=m>600</span>
</span></span><span class=line><span class=cl>profiler start --event Unsafe_AllocateMemory0 --duration <span class=m>600</span></span></span></code></pre></div></figure></div><p>通过上面的命令，能看到 MongoDB 和 netty 一直在申请使用内存。注意：早期的 mongodb client 确实有无法释放内存的 bug，但是在我们场景，长期观察会发现内存申请了逐渐释放了，没有持续增长。回到开头的 ContainerOOM 问题，可能一个原因是流量突增，MongoDB 申请了更多的内存导致 OOM，而不是因为内存不释放。</p><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>ts=2022-12-29 21:20:01;thread_name=ForkJoinPool.commonPool-worker-1;id=22;is_daemon=true;priority=1;TCCL=jdk.internal.loader.ClassLoaders$AppClassLoader@1d44bcfa
</span></span></span><span class=line><span class=cl><span class=go>    @jdk.internal.misc.Unsafe.allocateMemory()
</span></span></span><span class=line><span class=cl><span class=go>        at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:125)
</span></span></span><span class=line><span class=cl><span class=go>        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:332)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:243)
</span></span></span><span class=line><span class=cl><span class=go>        at java.net.Socket$SocketOutputStream.write(Socket.java:1035)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.SocketStream.write(SocketStream.java:99)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.InternalStreamConnection.sendMessage(InternalStreamConnection.java:426)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)
</span></span></span><span class=line><span class=cl><span class=go>        ………………………………
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.MongoClientExt$1.execute(MongoClientExt.java:42)
</span></span></span><span class=line><span class=cl><span class=go>        ………………………………
</span></span></span></code></pre></div></figure></div><p>另外，arthas 自带的 profiler 有时候经常追踪失败，可以切换到原始的 <a href=https://github.com/async-profiler/async-profiler target=_blank rel=noopener>async-profiler</a>
，用他来追踪“其他”内存分配比较有效。</p><p>总结 Java 进程内存占用：Total=heap + non-heap + 上面说的这个其他。</p><h2 id=jemalloc>jemalloc<a href=#jemalloc class=anchor aria-hidden=true>#</a></h2><p>jemalloc 是一个比 glibc malloc 更高效的内存池技术，在 Facebook 公司被大量使用，在 FreeBSD 和 FireFox 项目中使用了 jemalloc 作为默认的内存管理器。使用 jemalloc 可以使程序的内存管理性能提升，减少内存碎片。</p><p>比如 Redis 内存分配默认使用的 jemalloc，早期版本安装 redis 是需要手动安装 jemalloc 的，现在 redis 应该是在编译期内置好了。</p><p>原来使用 jemalloc 是为了分析内存占用，通过 jemalloc 输出当前内存分配情况，或者通过 diff 分析前后内存差，大概能看出内存都分给睡了，占了多少，是否有内存无法释放的情况。</p><p>后来参考了这个文章，把 glibc 换成 jemalloc 带来性能提升，降低内存使用，决定一试。</p><p>how we’ve reduced memory usage without changing any code：<a href=https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad target=_blank rel=noopener>https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad</a></p><p>Decreasing RAM Usage by 40% Using jemalloc with Python & Celery: <a href=https://zapier.com/engineering/celery-python-jemalloc/ target=_blank rel=noopener>https://zapier.com/engineering/celery-python-jemalloc/</a></p><p>一个服务，运行一周，观察效果。</p><p>使用 Jemalloc 之前：
<a href=./before-jemalloc.png><img src=/blog/java-memory/before-jemalloc_hu_b7da500d2cc5a630.webp width=1920 height=3304 decoding=async fetchpriority=auto loading=lazy alt=before id=h-rh-i-3></a></p><p>使用 Jemalloc 之后（同时调低了 Pod 内存）：
<a href=./after-jemalloc.png><img src=/blog/java-memory/after-jemalloc_hu_d417e08f73f2c90c.webp width=1920 height=3304 decoding=async fetchpriority=auto loading=lazy alt=after id=h-rh-i-4></a></p><p>注：以上结果未经生产长期检验。</p><h2 id=内存交还给操作系统>内存交还给操作系统<a href=#内存交还给操作系统 class=anchor aria-hidden=true>#</a></h2><p>注意：下面的操作，生产环境不建议这么干。</p><p>默认情况下，OpenJDK 不会主动向操作系统退还未用的内存（不严谨）。看第一张监控的图，会发现运行一段时间后，Pod 的内存使用量一直稳定在 80%&ndash;90%不再波动。</p><p>其实对于 Java 程序，浮动比较大的就是 heap 内存。其他区域 Code、Metaspace 基本稳定</p><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> 执行命令获取当前 heap 情况
</span></span><span class=line><span class=cl><span class=go>jhsdb jmap --heap --pid $(pgrep java)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=gp>#</span>以下为输出
</span></span><span class=line><span class=cl><span class=go>Attaching to process ID 7, please wait...
</span></span></span><span class=line><span class=cl><span class=go>Debugger attached successfully.
</span></span></span><span class=line><span class=cl><span class=go>Server compiler detected.
</span></span></span><span class=line><span class=cl><span class=go>JVM version is 17.0.5+8-LTS
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>using thread-local object allocation.
</span></span></span><span class=line><span class=cl><span class=go>ZGC with 4 thread(s)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Heap Configuration:
</span></span></span><span class=line><span class=cl><span class=go>   MinHeapFreeRatio         = 40
</span></span></span><span class=line><span class=cl><span class=go>   MaxHeapFreeRatio         = 70
</span></span></span><span class=line><span class=cl><span class=go>   MaxHeapSize              = 1287651328 (1228.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   NewSize                  = 1363144 (1.2999954223632812MB)
</span></span></span><span class=line><span class=cl><span class=go>   MaxNewSize               = 17592186044415 MB
</span></span></span><span class=line><span class=cl><span class=go>   OldSize                  = 5452592 (5.1999969482421875MB)
</span></span></span><span class=line><span class=cl><span class=go>   NewRatio                 = 2
</span></span></span><span class=line><span class=cl><span class=go>   SurvivorRatio            = 8
</span></span></span><span class=line><span class=cl><span class=go>   MetaspaceSize            = 22020096 (21.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   CompressedClassSpaceSize = 1073741824 (1024.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   MaxMetaspaceSize         = 17592186044415 MB
</span></span></span><span class=line><span class=cl><span class=go>   G1HeapRegionSize         = 0 (0.0MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Heap Usage:
</span></span></span><span class=line><span class=cl><span class=go> ZHeap          used 310M, capacity 710M, max capacity 1228M
</span></span></span></code></pre></div></figure></div><p>Java 内存不交还，几种情况：</p><ul><li><p>Xms 大于实际需要的内存，比如我们服务设置了 Xms768M，但是实际上只需要 256，高峰期也就 512，到不了 Xms 的值也就无所谓归还。
<a href=./java-heap-use.png><img src=/blog/java-memory/java-heap-use_hu_aecf7048f76716cf.webp width=896 height=287 decoding=async fetchpriority=auto loading=lazy alt=Xms id=h-rh-i-5></a></p></li><li><p>上面 jmap 的结果，可以看到 Java 默认的配置 MaxHeapFreeRatio=70，这个 70% Free 几乎很难达到。（另外注意 Xmx==Xms 的情况下这两个参数无效，因为他怎么扩缩都不会突破 Xms 和 Xmx 的限制）</p><div class=expressive-code><figure class="frame not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>  MinHeapFreeRatio         = 40
</span></span></span><span class=line><span class=cl><span class=go>  空闲堆空间的最小百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为 0 到 100，默认值为 40。如果 HeapFreeRatio &lt; MinHeapFreeRatio，则需要进行堆扩容，扩容的时机应该在每次垃圾回收之后。
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>  MaxHeapFreeRatio         = 70
</span></span></span><span class=line><span class=cl><span class=go>  空闲堆空间的最大百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为 0 到 100，默认值为 70。如果 HeapFreeRatio &gt; MaxHeapFreeRatio，则需要进行堆缩容，缩容的时机应该在每次垃圾回收之后。
</span></span></span></code></pre></div></figure></div></li></ul><p>对于 ZGC，默认是交还给操作系统的。可通过 <code>-XX:+ZUncommit -XX:ZUncommitDelay=300</code> 这两个参数控制（不再使用的内存最多延迟 300s 归还给 OS，线下环境可以改小点）。</p><p>经过调整后的服务，内存提交在 500&ndash;800M 之间浮动，不再是一条直线。</p><p><a href=./memory-dance.png><img src=/blog/java-memory/memory-dance_hu_5261cce6f6bab0aa.webp width=1839 height=645 decoding=async fetchpriority=auto loading=lazy alt=memory-dance id=h-rh-i-6></a></p><h2 id=内存分析工具速览>内存分析工具速览<a href=#内存分析工具速览 class=anchor aria-hidden=true>#</a></h2><p>使用 Java 自带工具 dump 内存。</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>jcmd &lt;pid&gt; GC.heap_dump &lt;file-path&gt;
</span></span><span class=line><span class=cl><span class=c1># 这个命令执行，JVM 会先触发 gc，然后再统计信息。</span>
</span></span><span class=line><span class=cl>jmap -dump:live,format<span class=o>=</span>b,file<span class=o>=</span>/opt/tomcat/logs/dump.hprof &lt;pid&gt;
</span></span><span class=line><span class=cl><span class=c1># dump all</span>
</span></span><span class=line><span class=cl>jmap -dump:format<span class=o>=</span>b,file<span class=o>=</span>/opt/tomcat/logs/dump.hprof &lt;pid&gt;</span></span></code></pre></div></figure></div><p>使用 jmap 输出内存占用概览。</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>jmap -histo <span class=m>1</span> <span class=p>|</span> head -n <span class=m>500</span></span></span></code></pre></div></figure></div><p>使用 async-profiler 追踪 native 内存分配，输出火焰图。</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>async-profiler/bin/asprof -d <span class=m>600</span> -e Unsafe_AllocateMemory0 -f /opt/tomcat/logs/unsafe_allocate.html &lt;pid&gt;</span></span></code></pre></div></figure></div><p>使用 vmtouch 查看和清理 Linux 系统文件缓存。</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看文件或文件夹占了多少缓存</span>
</span></span><span class=line><span class=cl>vmtouch /files
</span></span><span class=line><span class=cl>vmtouch /dir
</span></span><span class=line><span class=cl><span class=c1># 遍历文件夹输出详细占用</span>
</span></span><span class=line><span class=cl>vmtouch -v /dir
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 清空缓存</span>
</span></span><span class=line><span class=cl>vmtouch -e /dir</span></span></code></pre></div></figure></div><h3 id=pmap-查看内存内容>pmap 查看内存内容<a href=#pmap-查看内存内容 class=anchor aria-hidden=true>#</a></h3><p>使用 pmap 查看当前内存分配，如果找到了可疑的内存块，可以通过 gdb 尝试解析出内存块中的内容。</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># pmap 查看内存，先不要排序，便于找出连续的内存块（一般是 2 个一组）</span>
</span></span><span class=line><span class=cl><span class=c1># pmap -x &lt;pid&gt; | sort -n -k3</span>
</span></span><span class=line><span class=cl>pmap -x <span class=k>$(</span>pgrep java<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=c1># 举例说明在上面发现有 7f6737dff000 开头的内存块可能异常，一般都是一个或多个一组，是连续的内存</span>
</span></span><span class=line><span class=cl>cat /proc/<span class=k>$(</span>pgrep java<span class=k>)</span>/smaps &gt; logs/smaps.txt
</span></span><span class=line><span class=cl>gdb attach <span class=k>$(</span>pgrep java<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=c1># dump 的起始地址，基于上面 smaps.txt 找到的内容，地址加上 0x 前缀</span>
</span></span><span class=line><span class=cl>dump memory /opt/tomcat/logs/gdb-test.dump 0x7f6737dff000 0x7f6737e03000
</span></span><span class=line><span class=cl><span class=c1># 尝试将 dump 文件内容转成可读的 string，其中 -10 是过滤长度大于 10 的，也可以不过滤</span>
</span></span><span class=line><span class=cl>strings -10 /opt/tomcat/logs/gdb-test.dump
</span></span><span class=line><span class=cl><span class=c1># 如果幸运，能在上面的 strings 中找到你的 Java 类或 Bean 内容，如果不幸都是一堆乱码，可以尝试扩大 dump 内存块，多找几个连续的块试试</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># pmap 按大小降序排序并过滤大于 1000 KB 的项</span>
</span></span><span class=line><span class=cl>pmap -x <span class=k>$(</span>pgrep java<span class=k>)</span> <span class=p>|</span> awk <span class=s1>&#39;NR&gt;2 &amp;&amp; !/total/ {print $2, $0}&#39;</span> <span class=p>|</span> sort -k1,1nr <span class=p>|</span> cut -d<span class=s1>&#39; &#39;</span> -f2- <span class=p>|</span> awk <span class=s1>&#39;$2 &gt; 1000&#39;</span></span></span></code></pre></div></figure></div><h3 id=识别-linux-节点上的-cgroup-版本>识别 Linux 节点上的 cgroup 版本<a href=#识别-linux-节点上的-cgroup-版本 class=anchor aria-hidden=true>#</a></h3><p>cgroup 版本取决于正在使用的 Linux 发行版和操作系统上配置的默认 cgroup 版本。 要检查你的发行版使用的是哪个 cgroup 版本，请在该节点上运行 <code>stat -fc %T /sys/fs/cgroup/</code> 命令：</p><div class=expressive-code><figure class="frame is-terminal not-content"><figcaption class=header><span class=title></span></figcaption><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>stat -fc %T /sys/fs/cgroup/</span></span></code></pre></div></figure></div><p>对于 cgroup v2，输出为 cgroup2fs。</p><p>对于 cgroup v1，输出为 tmpfs。</p><h2 id=问题原因分析和调整>问题原因分析和调整<a href=#问题原因分析和调整 class=anchor aria-hidden=true>#</a></h2><p>回到开头问题，通过上面分析，2G 内存，RSS 其实占用 600M，为什么最终还是 ContainerOOM 了。</p><ol><li>kernel memory 为 0，排除 kernel 泄漏的原因。下面的参考资料里介绍了 kernel 泄露的两种场景。</li><li>Cache 很大，说明文件操作多。搜了一下代码，确实有很多 InputStream 调用没有显式关闭，而且有的 InputSteam Root 引用在 ThreadLocal 里，ThreadLocal 只 init 未 remove。 但是，ThreadLocal 的引用对象是线程池，池不回收，所以这部分可能会无法关闭，但是不会递增，但是 cache 也不能回收。
优化办法：ThreadLocal 中对象是线程安全的，无数据传递，直接干掉 ThreadLocal；显式关闭 InputStream。运行一周发现 cache 大约比优化前低 200&ndash;500M。
ThreadLocal 引起内存泄露是 Java 中很经典的一个场景，一定要特别注意。</li><li>一般场景下，Java 程序都是堆内存占用高，但是这个服务堆内存其实在 200-500M 之间浮动，我们给他分了 768M，从来没有到过这个值，所以调低 Xms。留出更多内存给 JNI 使用。</li><li>线下环境内存分配切换到 jemalloc，长期观察大部分效果可以，但是对部分应用基本没有效果。</li></ol><p>经过上述调整以后，线下环境 Pod 内存使用量由 1G 降到 600M 作用。线上环境内存使用量在 50%&ndash;80%之间根据流量大小浮动，原来是 85% 居高不小。</p><h2 id=不同-jvm-参数内存占用对比>不同 JVM 参数内存占用对比<a href=#不同-jvm-参数内存占用对比 class=anchor aria-hidden=true>#</a></h2><p>以下为少量应用实例总结出来的结果，应用的模型不同占用情况会有比较大差异，仅供对比参考。</p><table><thead><tr><th>基础参数</th><th>中低流量时内存占用（Xmx 6G）</th><th>高流量时内存占用</th></tr></thead><tbody><tr><td>Java 8 + G1</td><td>65%</td><td>85%</td></tr><tr><td>Java 17 + G1</td><td>60%</td><td>75%</td></tr><tr><td>Java 17 + ZGC</td><td>90%</td><td>95%</td></tr><tr><td>Java 21 + G1</td><td>40%</td><td>60%</td></tr><tr><td>Java 21 + ZGC</td><td>80%</td><td>90%</td></tr><tr><td>Java 21 + ZGC + UseStringDeduplication</td><td>85%</td><td>90%</td></tr><tr><td>Java 21 + ZGC + ZGenerational + UseStringDeduplication</td><td>75%</td><td>80%</td></tr></tbody></table><p>总结：</p><ol><li>G1 比 ZGC 占用内存明显减少。</li><li>Java 21 比 Java 8、17 占用内存明显偏少。</li><li>Java 21 ZGC 分代后确实能降低内存。</li><li>通过 <code>-XX:+UseStringDeduplication</code> 启用 String 去重后，有的应用能降低 10% 内存，有的几乎无变化。</li></ol><p>分享我们所使用的 Java 21 生产环境参数配置，仅供参考请根据自己应用情况选择性使用：</p><ul><li>-XX:InitialRAMPercentage=40.0 -XX:MaxRAMPercentage=70.0：按照百分比设置初始化和最大堆内存。内存充足的情况下建议设置为一样大。</li><li>-XX:+UseZGC -XX:+ZUncommit -XX:ZUncommitDelay=300 -XX:MinHeapFreeRatio=10 -XX:MaxHeapFreeRatio=30：促进 Java 内存更快交还给操作系统，但同时 CPU 可能偏高。</li><li>-XX:+ZGenerational：启用分代 ZGC，能降低内存占用。</li><li>-XX:+UseStringDeduplication：启用 String 去重，可能降低内存占用。</li><li>-Xss256k：降低线程内存占用，默认 1Mb，线程比较多的情况下这个占用还是很多的。谨慎设置。</li><li>-XX:+ParallelRefProcEnabled：多线程并行处理 Reference，减少 GC 的 Reference 数量，减少 Young GC 时间。</li></ul><p>关于 Java 8、17 和 21 不同 GC 更多维度的对比效果可参考： <a href=https://kstefanj.github.io/2023/12/13/jdk-21-the-gcs-keep-getting-better.html target=_blank rel=noopener>JDK 21: The GCs keep getting better</a>
。</p><h2 id=java-分析工具>Java 分析工具<a href=#java-分析工具 class=anchor aria-hidden=true>#</a></h2><ul><li>[推荐] 在线 GC 分析工具 <a href=https://gceasy.io/ target=_blank rel=noopener>GCeasy.io</a></li><li>[推荐] 在线 Thread 分析工具 <a href=https://fastthread.io/ target=_blank rel=noopener>FastThread.io</a></li><li>[推荐] 在线 Heap 分析工具 <a href=https://heaphero.io/ target=_blank rel=noopener>HeapHero.io</a></li><li>[推荐] 在线 jstack 分析工具 <a href=https://jstack.review/ target=_blank rel=noopener>jstack.review</a></li><li>[Beta] 可私有化部署 Online GC、Heap Dump、Thread、JFR 分析工具 <a href=https://github.com/eclipse/jifa target=_blank rel=noopener>eclipse/jifa</a></li></ul><h2 id=参考资料>参考资料<a href=#参考资料 class=anchor aria-hidden=true>#</a></h2><p>IOTDB 线上堆外内存泄漏问题排查：<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=195728187" target=_blank rel=noopener>https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=195728187</a></p><p>JVM 堆外内存问题定位： <a href=https://juejin.cn/post/6844904168549777421 target=_blank rel=noopener>https://juejin.cn/post/6844904168549777421</a></p><p>java 堆外内存泄漏排查： <a href=https://javakk.com/1158.html target=_blank rel=noopener>https://javakk.com/1158.html</a></p><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"><div class=last-modified><a href=https://github.com/xlabs-club/xlabs-club.github.io/commit/b8f43941168c955abc427f92d358c4a4574c2bef><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
Last updated on 2025年6月5日</a></div><div class=edit-page><a href=https://github.com/xlabs-club/xlabs-club.github.io/blob/main/content//blog/java-memory/index.md><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-edit-2"><path d="M17 3a2.828 2.828.0 114 4L7.5 20.5 2 22l1.5-5.5L17 3z"/></svg>
Edit this page on GitHub</a></div></div><div class="page-nav d-flex flex-column flex-sm-row"><div class="card w-100"><div class="card-body d-flex"><div class="d-flex flex-column justify-content-center"><svg class="icon icon-tabler icon-tabler-arrow-left" width="20" height="20" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 12h14"/><path d="M5 12l6 6"/><path d="M5 12l6-6"/></svg></div><div class="d-flex flex-column"><div class=text-body-secondary>Prev</div><a href=/blog/k8s-pid-limiting-oom/ class="stretched-link text-reset text-decoration-none">K8S 容器 PID 限制引起的 Java OutOfMemoryError</a></div></div></div><div class=m-2></div><div class="card text-end w-100"><div class="card-body d-flex justify-content-end"><div class="d-flex flex-column"><div class=text-body-secondary>Next</div><a href=/blog/code-server/ class="stretched-link text-reset text-decoration-none">使用 Visual Studio Code 搭建多用户远程 IDE</a></div><div class="d-flex flex-column justify-content-center"><svg class="icon icon-tabler icon-tabler-arrow-right" width="20" height="20" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 12h14"/><path d="M13 18l6-6"/><path d="M13 6l6 6"/></svg></div></div></div></div></main></div></div></div></div><footer class="footer text-muted"><div class=container-fluid><div class=row><div class="col-lg-8 text-center text-lg-start"><ul class=list-inline><li class=list-inline-item><a class=text-muted href=/>Powered by Hugo, and based on Doks</a></li></ul></div><div class="col-lg-8 text-center text-lg-end"><ul class=list-inline><li class=list-inline-item>Give Me a <a class=text-muted href=https://github.com/xlabs-club/>Star</a></li></ul></div></div></div></footer><script async src=/js/bootstrap.c673c84373e66019f7895efc74b3b3b940071832aaa8a85e73218087d535e71b.js integrity="sha256-xnPIQ3PmYBn3iV78dLOzuUAHGDKqqKhecyGAh9U15xs="></script><script async src=/js/app.d05c432390da35574e3ded866e7272ad59de79e668c4a70834a8dc52167d6982.js integrity="sha256-0FxDI5DaNVdOPe2GbnJyrVneeeZoxKcINKjcUhZ9aYI="></script><script async src=/js/docsearch.c10d12b41b383a226ea1accfc11112a4463d0dc30116d335216301f8049c938c.js integrity="sha256-wQ0StBs4OiJuoazPwRESpEY9DcMBFtM1IWMB+ASck4w="></script></body></html>